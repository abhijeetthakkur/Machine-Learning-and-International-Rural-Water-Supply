{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading all files and changing there name to \"correded\",\"non-correded\",\"damaged\" and \"non-damaged\" according to the the csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "# import csv\n",
    "# import os\n",
    "# path = 'BGS_Images'\n",
    "# with open('BGS_name.csv') as csvDataFile:\n",
    "#     csvReader = csv.reader(csvDataFile)\n",
    "#     i=0\n",
    "#     for row in csvReader:\n",
    "#         if i==0:\n",
    "#             i=2\n",
    "#             continue\n",
    "#         filename=row[0]\n",
    "#         filepath=os.path.join(path,filename)\n",
    "#         all_files=os.listdir(filepath)\n",
    "#         for x in all_files:\n",
    "#             newfilename=x.split(\".\")[0]+\"_\"+row[1]+\"_\"+row[2]+\".\"+x.split(\".\")[1]\n",
    "#             newfilename_path=os.path.join(filepath,newfilename)\n",
    "#             oldfilename_path=os.path.join(filepath,x)\n",
    "#             os.rename(oldfilename_path,newfilename_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting image to gray scale and then making np array of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "307\n",
      "(306, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# train images\n",
    "import csv\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "\n",
    "\n",
    "\n",
    "X_train_data = []\n",
    "\n",
    "# dir_files =\"train/\"\n",
    "dir_files = \"train/\"\n",
    "dirs = os.listdir(dir_files)\n",
    "for img in dirs:\n",
    "        if img.endswith(\".JPG\"):\n",
    "            img_array = cv2.imread(os.path.join(dir_files,img), cv2.IMREAD_GRAYSCALE)\n",
    "            resized_image = cv2.resize(img_array, (28, 28))\n",
    "    #         img_flatten = resized_image.reshape(-1)\n",
    "            X_train_data.append(resized_image)\n",
    "np_X_train_data = np.array(X_train_data)        \n",
    "        \n",
    "print(np_X_train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n",
      "(120, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# test images\n",
    "X_test_data = []\n",
    "dir_files =\"test/\"\n",
    "print(len(os.listdir(dir_files)))\n",
    "\n",
    "for img in os.listdir(dir_files):\n",
    "        img_array = cv2.imread(os.path.join(dir_files,img), cv2.IMREAD_GRAYSCALE)\n",
    "        resized_image = cv2.resize(img_array, (28, 28))\n",
    "#         img_flatten = resized_image.reshape(-1)\n",
    "        X_test_data.append(resized_image)\n",
    "np_X_test_data = np.array(X_test_data)        \n",
    "        \n",
    "print(np_X_test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating labels for train and test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306\n",
      "306\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os, sys\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "Y_train_data = []\n",
    "dir_files = glob.glob(\"train/**.JPG\")\n",
    "print(len(dir_files))\n",
    "for ii in dir_files:\n",
    "    if ii.find('_Not Corroded_Not Damaged')!=-1:\n",
    "        Y_train_data.append(2)\n",
    "    elif ii.find('_Not Corroded_Damaged')!=-1:\n",
    "        Y_train_data.append(1)\n",
    "    elif ii.find('_Corroded_Not Damaged')!=-1:\n",
    "        Y_train_data.append(3)\n",
    "    elif ii.find('_Corroded_Damaged')!=-1:\n",
    "        Y_train_data.append(0)\n",
    "\n",
    "np_Y_train_data = np.array(Y_train_data) \n",
    "print(len(Y_train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n",
      "120\n"
     ]
    }
   ],
   "source": [
    "Y_test_data = []\n",
    "dir_files = glob.glob(\"test/**.JPG\")\n",
    "print(len(dir_files))\n",
    "for ii in dir_files:\n",
    "    if ii.find('_Not Corroded_Not Damaged')!=-1:\n",
    "        Y_test_data.append(2)\n",
    "    elif ii.find('_Not Corroded_Damaged')!=-1:\n",
    "        Y_test_data.append(1)\n",
    "    elif ii.find('_Corroded_Not Damaged')!=-1:\n",
    "        Y_test_data.append(3)\n",
    "    elif ii.find('_Corroded_Damaged')!=-1:\n",
    "        Y_test_data.append(0)\n",
    "\n",
    "np_Y_test_data = np.array(Y_test_data) \n",
    "print(len(Y_test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 50 # how much data of each class\n",
    "c1, c2, c3, c4= 2, 1, 3, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of training subset: (53, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "index_of_c1 = np.argwhere(np_Y_train_data == c1).ravel() # index of 2s\n",
    "index_of_c1 = np.random.choice(index_of_c1, replace=False)\n",
    "\n",
    "index_of_c2 = np.argwhere(np_Y_train_data == c2).ravel() # index of 3s\n",
    "index_of_c2 = np.random.choice(index_of_c2, replace=False)\n",
    "\n",
    "index_of_c3 = np.argwhere(np_Y_train_data == c3).ravel() # index of 4s\n",
    "index_of_c3 = np.random.choice(index_of_c3, replace=False)\n",
    "\n",
    "index_of_c4 = np.argwhere(np_Y_train_data == c4).ravel() # index of 4s\n",
    "index_of_c4 = np.random.choice(index_of_c4, N, replace=False)\n",
    "\n",
    "x_train = np_X_train_data[np.hstack((index_of_c1, index_of_c2, index_of_c3, index_of_c4))] # subsample\n",
    "y_train = np_Y_train_data[np.hstack((index_of_c1, index_of_c2, index_of_c3, index_of_c4))] # subsample\n",
    "\n",
    "print(\"size of training subset: %s\" % str(x_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 3)\n"
     ]
    }
   ],
   "source": [
    "as_rgb = lambda im: np.tile(np.expand_dims(im,2),(1,1,3))\n",
    "\n",
    "#example\n",
    "print(as_rgb(x_train[0,:,:]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "/anaconda3/lib/python3.7/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53, 299, 299, 3)\n"
     ]
    }
   ],
   "source": [
    "import skimage\n",
    "resize = lambda im: skimage.transform.resize(as_rgb(im), (299, 299), preserve_range=False, order=0)\n",
    "\n",
    "new_x_train = np.concatenate([[resize(x_train[i,:,:])] for i in range(x_train.shape[0])])\n",
    "print(new_x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.inception_v3 import InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0718 13:04:21.246817 4627367360 deprecation.py:506] From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "inception = InceptionV3(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    pooling='avg',\n",
    "    input_shape=(299,299,3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of training subset: (53, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "index_of_c1 = np.argwhere(np_Y_test_data == c1).ravel() # index of 2s\n",
    "index_of_c1 = np.random.choice(index_of_c1, replace=False)\n",
    "\n",
    "index_of_c2 = np.argwhere(np_Y_test_data == c2).ravel() # index of 3s\n",
    "index_of_c2 = np.random.choice(index_of_c2, replace=False)\n",
    "\n",
    "index_of_c3 = np.argwhere(np_Y_test_data == c3).ravel() # index of 4s\n",
    "index_of_c3 = np.random.choice(index_of_c3, replace=False)\n",
    "\n",
    "index_of_c4 = np.argwhere(np_Y_test_data == c4).ravel() # index of 4s\n",
    "index_of_c4 = np.random.choice(index_of_c4, N, replace=False)\n",
    "\n",
    "x_test = np_X_test_data[np.hstack((index_of_c1, index_of_c2, index_of_c3, index_of_c4))] # subsample\n",
    "y_test = np_Y_test_data[np.hstack((index_of_c1, index_of_c2, index_of_c3, index_of_c4))] # subsample\n",
    "\n",
    "print(\"size of training subset: %s\" % str(x_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "/anaconda3/lib/python3.7/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53, 299, 299, 3)\n"
     ]
    }
   ],
   "source": [
    "new_x_test = np.concatenate([[resize(x_test[i,:,:])] for i in range(x_test.shape[0])])\n",
    "print(new_x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53, 2048)\n"
     ]
    }
   ],
   "source": [
    "training_features = inception.predict(new_x_train)\n",
    "print(training_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53, 2048)\n"
     ]
    }
   ],
   "source": [
    "test_features = inception.predict(new_x_test)\n",
    "print(test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC as Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = Classifier()\n",
    "\n",
    "clf.fit(training_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = clf.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.943396\n",
      "successfully classified: 50/53\n"
     ]
    }
   ],
   "source": [
    "acc = clf.score(test_features, y_test)\n",
    "print(\"accuracy: %f\" % acc)\n",
    "print(\"successfully classified: %d/%d\" % (np.sum(y_predict == y_test), len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "classifiers = {\n",
    "    'svm': SVC, \n",
    "    'sgd': SGDClassifier, \n",
    "    'linearsvm': LinearSVC,\n",
    "    'decisiontree': DecisionTreeClassifier,\n",
    "    'naivebayes': GaussianNB\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifer: svm\n",
      "  training accuracy: 0.943396\n",
      "  test accuracy: 0.943396\n",
      "classifer: sgd\n",
      "  training accuracy: 1.000000\n",
      "  test accuracy: 0.943396\n",
      "classifer: linearsvm\n",
      "  training accuracy: 1.000000\n",
      "  test accuracy: 0.943396\n",
      "classifer: decisiontree\n",
      "  training accuracy: 1.000000\n",
      "  test accuracy: 0.867925\n",
      "classifer: naivebayes\n",
      "  training accuracy: 1.000000\n",
      "  test accuracy: 0.943396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "for name, Classifer in classifiers.items():\n",
    "    clf = Classifer()\n",
    "    clf.fit(training_features, y_train)\n",
    "    train_acc = clf.score(training_features, y_train)\n",
    "    test_acc  = clf.score(test_features,  y_test)\n",
    "    print(\"classifer: %s\\n  training accuracy: %f\\n  test accuracy: %f\" % (name, train_acc, test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
